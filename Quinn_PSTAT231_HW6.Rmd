---
title: "Homework 6"
author: "Olivia Quinn"
date: "5/23/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

### Loading Packages 

```{r}
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("vip")
library(vip)
#install.packages("randomForest")
library(randomForest)
#install.packages("xgboost")
library(xgboost)
#install.packages("ranger")
library(ranger)

library(tidyverse)
library(tidymodels)
library(ISLR)
library(janitor)
library(corrplot)

```

## Tree-Based Models
 
The goal of this assignment is to build a statistical learning model that can predict the **primary type** of a Pokémon based on its generation, legendary status, and six battle statistics.
 
### Exercise 1
 
Read in the data, use `clean_names()`, filter out the rarer Pokémon types, convert `type_1` and `legendary` to factors:
 
```{r}
pokemon <- read_csv("data/Pokemon.csv")
pokemon <- clean_names(pokemon)

dat <- pokemon %>% 
  filter(type_1 %in% c("Bug", "Fire", "Grass", "Normal", "Water", "Psychic"))

dat <- dat %>%
  mutate(type_1 = as.factor(type_1)) %>% 
  mutate(legendary = as.factor(legendary)) %>% 
  mutate(generation = as.factor(generation))
```


Initial split + stratify on the outcome variable:
 
```{r}
set.seed(24)
poke_split <- initial_split(dat, prop = 0.70, strata = type_1)
poke_train <- training(poke_split)
poke_test <- testing(poke_split)

dim(poke_train)
dim(poke_test)
```


Fold the training set using *v*-fold cross-validation + stratify on the outcome variable:
 
```{r}
poke_folds <- vfold_cv(poke_train, v = 5, strata = type_1)
poke_folds
```


Recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def` (dummy the factors + center and scale all predictors):
 
```{r}
poke_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, data = poke_train) %>% 
  step_dummy(legendary) %>% 
  step_dummy(generation) %>% 
  step_normalize(all_predictors())
```


### Exercise 2

Correlation matrix:
 
```{r}
poke_train %>% 
  select(is.numeric) %>% 
  cor() %>% 
  corrplot(type = 'lower', diag = FALSE, 
           method = 'color')
```
  
I chose to only include the numeric variables in this correlation matrix because the name and type variables are factors without an inherent ordering. They cannot be converted to a nominal or ordinal scale. 
  
There are positive relationships between most pokemon stats: i.e. between pokemon hit points, attack, defense, special attack, special defense, and speed. It makes sense that pokemon with greater attack stats would also be better defenders!    
 
### Exercise 3

Decision tree model and workflow + tune 'cost_complexity' with `range = c(-3, -1)` + optimize 'roc_auc':

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart")

class_tree_spec <- tree_spec %>%
  set_mode("classification")

class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(poke_recipe)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  class_tree_wf, 
  resamples = poke_folds, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)

autoplot(tune_res)
```
  
A single decision tree performs best with a small/medium sized complexity penalty, with a peak around ~0.02.
  
### Exercise 4
 
The ROC_AUC of the best-performing pruned tree (on the folds) is 0.64.

```{r}
tune_res %>% 
  collect_metrics() %>% 
  arrange(-mean)
```

### Exercise 5
  
Fitting and visualizing the best-performing pruned decision tree with training data:
  
Fit:
```{r}
best_complexity <- select_best(tune_res)

class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)

class_tree_final_fit <- fit(class_tree_final, data = poke_train)
```

Visualize:
```{r}
class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot(type = 4)
```
  

### Exercise 5

Fandom forest model and workflow + `ranger` engine and `importance = "impurity"` + tune `mtry`, `trees`, and `min_n`:
  
- 'mtry' refers to the number of predictors which each tree will be trained on. 
  
- 'trees' refers to the total number of trees to be fit. 
  
- 'min_n' refers to the minimum number of observations in a node that are required for the node to be split further. 
  
For the regular grid, a model with 'mtry = 8' would represent a tree trained on all 8 predictors, which would be a bagging model. 
  
```{r}
rf_spec <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = tune(), trees = tune(), min_n = tune())) %>%
  add_recipe(poke_recipe)

rf_grid <- grid_regular(mtry(range = c(1, 8)), 
                        trees(range = c(100,1000)), 
                              min_n(range = c(2,10)),
                                levels = 8)

```


### Exercise 6

Tuned model with 'roc_auc' metric: 

```{r}
# saved and loaded because took 15 minutes to run! 

#rf_tune_res <- tune_grid(
#  rf_wf, 
#  resamples = poke_folds, 
#  grid = rf_grid, 
#  metrics = metric_set(roc_auc))

#saveRDS(rf_tune_res, file = "rf_tune_res.rds")

rf_tune_res <- readRDS(file = "rf_tune_res.rds")

autoplot(rf_tune_res)

```
  
The tuning results show that the best performance is gained by using at least two predictors, at least 100 trees, and larger minimal node sizes. Different ranges for each of these hyperparameters might yield better performances, but I will keep them as is. Roc_auc appears to level off around ~0.70, however, so tuning more may not be more helpful. 
  
### Exercise 7

The ROC_AUC of the best-performing random forest model is 0.72. 

```{r}
rf_tune_res %>% 
  collect_metrics() %>% 
  arrange(-mean)
```


### Exercise 8

Variable importance plot with best-performing random forest model and training:

```{r}
best_rf_tune <- select_best(rf_tune_res, metric = "roc_auc")

rf_final <- finalize_workflow(rf_wf, best_rf_tune)

rf_final_fit <- fit(rf_final, data = poke_train)

rf_final_fit%>%
  pull_workflow_fit()%>%
  vip()
```

  
The attack/defense variables are most useful, which is what I expected given the correlation matrix and my priors about pokemon. Special attack looks like it is the most useful predictor. The generation of pokemon seems to not be very useful at all in predicting primary type. 


### Exercise 9
 
Boosted tree model and workflow (`xgboost` engine) + tune `trees`: 
 
```{r}
boost_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification")

boost_wf <- workflow() %>%
  add_model(boost_spec %>% set_args(trees = tune())) %>%
  add_recipe(poke_recipe)

boost_grid <- grid_regular(trees(range = c(10,2000)),levels = 10)

boost_tune_res <- tune_grid(
  boost_wf, 
  resamples = poke_folds, 
  grid = boost_grid, 
  metrics = metric_set(roc_auc))

autoplot(boost_tune_res)

```
 
Accuracy peaks at about 500 trees, but decreases with more. The ROC_AUC of the best-performing boosted tree model is 0.688.
  
```{r}
boost_tune_res %>% 
  collect_metrics() %>% 
  arrange(-mean)
```

### Exercise 10

Compare ROC AUC values for best-performing pruned tree, random forest, and boosted tree models:

```{r}
#need to combine all three res models by roc_auc... which select_best doesn't pull out for some reason 
tune_res_compare <-  show_best(tune_res, metric = "roc_auc", n = 1)
rf_compare <- show_best(rf_tune_res, metric = "roc_auc", n = 1)
boost_compare <- show_best(boost_tune_res, metric = "roc_auc", n = 1)

AUCs <- c(tune_res_compare$mean, rf_compare$mean, 
                boost_compare$mean)
models <- c("Pruned", "RF", "Boosted")
results <- tibble(AUCs = AUCs, models = models)
results %>% 
  arrange(-AUCs)
```

Fit to RF model to the testing set:
```{r}
augment(rf_final_fit, new_data = poke_test) %>% 
  roc_auc(type_1, estimate = c(.pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Psychic, .pred_Water))
```
Overall AUC value of the best-performing model on the testing set: 0.71  
  
ROC curve:
```{r}
augment(rf_final_fit, new_data = poke_test) %>% 
  roc_curve(type_1, estimate = c(.pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Psychic, .pred_Water)) %>% 
  autoplot()
```

Heatmap:
```{r}
augment(rf_final_fit, new_data = poke_test) %>%
  conf_mat(truth = type_1, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
```

The random forest model is best at predicting the Normal and Bug type Pokemon, and is worst at predicting the Water and Grass types.

## For 231 Students

### Exercise 11

Random forest model predicting abalone 'age' + tune `mtry`, `min_n`, and `trees`:  
  
Dat & CV: 
```{r}
abalone <- read_csv("data/abalone.csv")

abalone <- abalone %>% 
  mutate(age = rings + 1.5)

set.seed(24)

abalone_split <- initial_split(abalone, prop = 0.80,
                                strata = age)
abalone_train <- training(abalone_split)
abalone_test <- testing(abalone_split)

abalone_folds <- vfold_cv(abalone_train, v = 5, strata = age)

```

RF model:
```{r}
#leave out rings
abalone_recipe <- recipe(age ~ type + longest_shell + diameter + height + whole_weight + shucked_weight + viscera_weight + shell_weight, data = abalone_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())

abalone_rf_spec <- rand_forest() %>%
  set_engine("randomForest", importance = TRUE) %>%
  set_mode("regression")

abalone_rf_wf <- workflow() %>%
  add_model(abalone_rf_spec %>% set_args(mtry = tune(), trees = tune(), min_n = tune())) %>%
  add_recipe(abalone_recipe)

abalone_grid <- grid_regular(mtry(range = c(1, 8)), 
                        trees(range = c(10,200)), 
                              min_n(range = c(10,100)),
                                levels = 8)
```

Tune: 
```{r}
#abalone_rf_tune_res <- tune_grid(
 # abalone_rf_wf, 
 # resamples = abalone_folds, 
 # grid = abalone_grid, 
 # metrics = metric_set(rmse))

#saveRDS(abalone_rf_tune_res, file = "abalone_rf_tune_res.rds")

abalone_rf_tune_res <- readRDS(file = "abalone_rf_tune_res.rds")

autoplot(abalone_rf_tune_res)
```

Fit best on full training set:
```{r}
best_abalone_tune <- select_best(abalone_rf_tune_res, metric = "rmse")

abalone_rf_final <- finalize_workflow(abalone_rf_wf, best_abalone_tune)

abalone_rf_final_fit <- fit(abalone_rf_final, data = abalone_train)
```

Testing performance:  
  
RMSE on the test set is 2.13. Overall the fitted line looks accurate to the data;  however, there appears to be non-constant variance in the residuals, higher at larger values for age. This might be due to the large range in ages, or the impact of a factor predictor, like type, which I did not interact but probably could have.

```{r}
augment(abalone_rf_final_fit, new_data = abalone_test) %>%
  rmse(truth = age, estimate = .pred)

augment(abalone_rf_final_fit, new_data = abalone_test) %>%
  ggplot(aes(age, .pred)) +
  geom_abline() +
  geom_point(alpha = 0.5)
```





